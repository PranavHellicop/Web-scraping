{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from csv import writer\n",
    "from urllib.parse import urljoin\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to fix the ssleoferror\n",
    "\n",
    "import requests\n",
    "sess = requests.Session()\n",
    "adapter = requests.adapters.HTTPAdapter(max_retries = 20)\n",
    "sess.mount('http://', adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e888ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(397,476):  \n",
    "    \n",
    "    print('scraping page {}....'.format(j))\n",
    "    res = sess.get('https://jamaicaclassifiedonline.com/?s=&category=cars&category=cars&type=&title=&desc=&price=&currency=&place=&parish=&recent=&sort=&hasimages=&orderby=modified_date&page={}'.format(j))\n",
    "\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "    car_el = soup.select('div.row>div[class=\"col l3 s12 m6\"]>div>a')\n",
    "\n",
    "    links = []\n",
    "    for i in car_el:\n",
    "        link = i.get('href')\n",
    "        links.append(link)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    with open('cars_jamclassified.csv','a',encoding ='utf8',newline = '') as file:\n",
    "        writer_file = writer(file)\n",
    "        header = ['ID','Title','Make','Model','Location','Year','Price','Phone','Additional_Info','Fuel Type','Doors','Transmission','Body Type','Driver Side','Condition']\n",
    "        writer_file.writerow(header) \n",
    "\n",
    "        for k in links:\n",
    "            print('scraping car link {}'.format(k))\n",
    "            res = sess.get(k)\n",
    "\n",
    "            soup = BeautifulSoup(res.text,'html.parser')\n",
    "            try:\n",
    "                title = soup.find('h1',attrs = {'id':'title'}).text.strip()\n",
    "            except :\n",
    "                title = \"NA\"\n",
    "\n",
    "            try:\n",
    "                location = soup.select('div.card-title>div>span')[0].text.strip()\n",
    "            except :\n",
    "                location = \"NA\"\n",
    "\n",
    "            try:    \n",
    "                price = soup.select('div.card-title>h1>span>span')[0].text.strip()\n",
    "            except :\n",
    "                price = \"NA\"\n",
    "\n",
    "            try:\n",
    "                phone = \"'\" + soup.find('a',attrs = {'href':'#modal-call'}).text.strip()\n",
    "            except:\n",
    "                phone = \"NA\"\n",
    "\n",
    "            try:\n",
    "                add_info = soup.select('div#item-description>div>div')[0].text.strip()\n",
    "            except :\n",
    "                add_info = \"NA\"\n",
    "\n",
    "            lis = soup.select('ul.collection.with-header>li')\n",
    "\n",
    "            ad_id =\"\"\n",
    "            year =\"\"\n",
    "            make=\"\"\n",
    "            model=\"\"\n",
    "            fuel_type =\"\"\n",
    "            doors =\"\"\n",
    "            transmission =\"\"\n",
    "            body_type=\"\"\n",
    "            driver_side=\"\"\n",
    "            condition =\"\" \n",
    "\n",
    "            for i in lis[2:]:\n",
    "\n",
    "\n",
    "                obj = re.compile(r'ID:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match != None:\n",
    "                    ad_id = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Year:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    year = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Make:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    make= i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Model:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    model= i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Fuel Type:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    fuel_type = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Doors:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    doors = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Transmission:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    transmission = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Body Type:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    body_type= i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Driver Side:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    driver_side = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Condition:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    condition = i.text.strip()\n",
    "\n",
    "            if ad_id == \"\":\n",
    "                ad_id = 'NA'\n",
    "            if year ==\"\":\n",
    "                year = 'NA'\n",
    "            if make == \"\":\n",
    "                make = 'NA'\n",
    "            if model == \"\":\n",
    "                model= 'NA'\n",
    "            if fuel_type == \"\":\n",
    "                fuel_type = 'NA'\n",
    "            if doors == \"\":\n",
    "                doors = 'NA'\n",
    "            if transmission== \"\":\n",
    "                transmission = 'NA'\n",
    "            if body_type == \"\":\n",
    "                body_type = 'NA'\n",
    "            if driver_side == \"\":\n",
    "                driver_side= 'NA'\n",
    "            if condition == \"\":\n",
    "                condition = 'NA'\n",
    "\n",
    "\n",
    "            rows = [ad_id,title,make,model,location,year,price,phone,add_info,fuel_type,doors,transmission,body_type,driver_side,condition]\n",
    "\n",
    "            writer_file.writerow(rows)\n",
    "\n",
    "            print('Finished writing link {}, moving on to the next link'.format(k))\n",
    "            \n",
    "            time.sleep(2)\n",
    "            \n",
    "        print('Finished extracting Page {}'.format(j))\n",
    "\n",
    "print('Done! Extracted all the information')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe871b0",
   "metadata": {},
   "source": [
    "## Extracting Images for each cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,476):  \n",
    "    \n",
    "    print('scraping page {}....'.format(j))\n",
    "    res = sess.get('https://jamaicaclassifiedonline.com/?s=&category=cars&category=cars&type=&title=&desc=&price=&currency=&place=&parish=&recent=&sort=&hasimages=&orderby=modified_date&page={}'.format(j))\n",
    "\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "    car_el = soup.select('div.row>div[class=\"col l3 s12 m6\"]>div>a')\n",
    "\n",
    "    links = []\n",
    "    for i in car_el:\n",
    "        link = i.get('href')\n",
    "        links.append(link)   \n",
    "        \n",
    "    for i in links:\n",
    "        print('getting images from {}'.format(i)'\\n')\n",
    "        res = sess.get(i)\n",
    "\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "        ad_id = soup.select('ul.collection.with-header>li:nth-child(2)>div')[0].text.strip()\n",
    "        image = soup.select('div.col.s12.m8.l8 > div > div > div > a > img')\n",
    "\n",
    "\n",
    "        try:  \n",
    "            for i in image:\n",
    "\n",
    "                im_el = i.get('src')\n",
    "                with open('img_jamclassified.csv','a',encoding ='utf8',newline = \"\") as file:\n",
    "                    writer_file = writer(file)\n",
    "                    img_id= [ad_id,im_el]\n",
    "                    writer_file.writerow(img_id)\n",
    "                \n",
    "        except:\n",
    "            im_el = \"NA\"\n",
    "            with open('img_jamclassified.csv','a',encoding ='utf8',newline = \"\") as file:\n",
    "                writer_file = writer(file)\n",
    "                img_id= [ad_id,im_el]\n",
    "                writer_file.writerow(img_id)\n",
    "\n",
    "print('Done! Got all the image links')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c00a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "684c91f4",
   "metadata": {},
   "source": [
    "# Extracting House Ads from https://cbjamaica.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b303b03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(133,214):\n",
    "    \n",
    "    with open('houses_cbjam.csv','a',encoding ='utf8',newline = '') as file:\n",
    "        \n",
    "        writer_file = writer(file)\n",
    "        header = ['ID','Title','Location','Latitude','Longitude','Price','Description','Up Price','Static Price','Down Price','Bedrooms','Bathrooms','Area','Interior','Exterior','Other','Site Influences']\n",
    "        writer_file.writerow(header)  \n",
    "        \n",
    "        print('scraping page {}....'.format(j))\n",
    "\n",
    "        res = sess.get('https://cbjamaica.com/property-search?page={}&limit=12&rent_sale=both&bed=&bath=&property_type=House&parish=&town=&currency=&keyword=&range=&agent=&agent_name=&similar=&tagOnly=&search_field=&idSearch=&property_amenities=&statuses=&price_range=&lattitude=&longitude='.format(j))\n",
    "\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "        car_el = soup.select('div#products>div>div>div:nth-child(3)>div>h2>a')\n",
    "\n",
    "        links = []\n",
    "        for i in car_el:\n",
    "            link = i.get('href')\n",
    "            links.append(link)\n",
    "\n",
    "\n",
    "\n",
    "        for k in links:\n",
    "            \n",
    "            print('scraping car link {}'.format(k))\n",
    "            res = sess.get(k)\n",
    "\n",
    "            soup = BeautifulSoup(res.text,'html.parser')\n",
    "            \n",
    "            try:\n",
    "                ad_id = soup.select('div.sale-offer-part > h1>span')[0].text.strip()\n",
    "            except:\n",
    "                ad_id = \"NA\"\n",
    "\n",
    "            try:\n",
    "                title = soup.select('div.sale-offer-part > h1')[0].text.strip()\n",
    "                title = \" \".join(title.split())  \n",
    "            except:\n",
    "                title = \"NA\"\n",
    "\n",
    "            try:\n",
    "                loc = soup.select('div.sale-offer-part>p')[0].text.strip()\n",
    "                loc = \" \".join(loc.split())\n",
    "            except:\n",
    "                loc = \"NA\"\n",
    "\n",
    "            try:\n",
    "                lat = soup.select('div#map-main')[0].get('data-lat')\n",
    "            except:\n",
    "                lat = \"NA\"\n",
    "            try:\n",
    "                lon = soup.select('div#map-main')[0].get('data-long')\n",
    "            except:\n",
    "                lon = \"NA\"\n",
    "\n",
    "            try:\n",
    "                price = soup.find('h2',attrs = {'class':'property-price'}).text.strip()\n",
    "            except:\n",
    "                price = \"NA\"\n",
    "\n",
    "            try:\n",
    "                descr = soup.find('div',attrs = {'class':'property-description'}).text.strip()\n",
    "            except:\n",
    "                descr = \"NA\"\n",
    "\n",
    "            try:\n",
    "                up_price = soup.find('li',attrs = {'class':'up-price'}).text.strip()\n",
    "            except:\n",
    "                up_price = \"NA\"\n",
    "\n",
    "            try:\n",
    "                static_price = soup.find('li',attrs = {'class':'static-price'}).text.strip()\n",
    "            except:\n",
    "                static_price = \"NA\"\n",
    "\n",
    "            try:    \n",
    "                down_price = soup.find('li',attrs = {'class':'down-price'}).text.strip()\n",
    "            except:\n",
    "                down_price = \"NA\"\n",
    "\n",
    "            lis= soup.select('div.single-property-info-bottom>ul>li')\n",
    "\n",
    "            bedrooms = \"\"\n",
    "            bathrooms = \"\"\n",
    "            area = \"\"\n",
    "\n",
    "            for i in lis:\n",
    "                obj = re.compile(r'Bedrooms',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match != None:\n",
    "                    bedrooms= i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Bathrooms',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match != None:\n",
    "                    bathrooms= i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Sq Ft',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match != None:\n",
    "                    area= i.text.strip()\n",
    "\n",
    "            if bedrooms == \"\":\n",
    "                bedrooms = 'NA'\n",
    "            if bathrooms == \"\":\n",
    "                bathrooms = 'NA'\n",
    "            if area == \"\":\n",
    "                area = 'NA'\n",
    "\n",
    "\n",
    "            amen = soup.select('div.tab-inner-points>div>div>div')\n",
    "\n",
    "            interior = \"\"\n",
    "            exterior = \"\"\n",
    "            other = \"\"\n",
    "            site = \"\"\n",
    "\n",
    "            for i in amen:\n",
    "                obj = re.compile(r'Interior',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match != None:\n",
    "                    interior= i.text.strip()\n",
    "                    interior = \", \".join(interior.split())\n",
    "\n",
    "                obj = re.compile(r'Exterior',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match != None:\n",
    "                    exterior= i.text.strip()\n",
    "                    exterior = \", \".join(exterior.split())\n",
    "\n",
    "                obj = re.compile(r'Other',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match != None:\n",
    "                    other= i.text.strip()\n",
    "                    other = \", \".join(other.split())\n",
    "\n",
    "                obj = re.compile(r'Site Influences',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match != None:\n",
    "                    site= i.text.strip()\n",
    "                    site = \", \".join(site.split())\n",
    "\n",
    "            if interior == \"\":\n",
    "                interior = 'NA'\n",
    "            if exterior == \"\":\n",
    "                exterior = 'NA'\n",
    "            if other == \"\":\n",
    "                other = 'NA'\n",
    "            if site == \"\":\n",
    "                site = 'NA'\n",
    "                \n",
    "            rows = [ad_id,title,loc,lat,lon, \n",
    "                    price,descr,up_price,static_price,down_price,bedrooms,bathrooms,area,interior,exterior,other,site]\n",
    "\n",
    "            writer_file.writerow(rows)\n",
    "            \n",
    "            print('Finished writing link {}, moving on to the next link'.format(k))\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "        print('Finished extracting Page {}'.format(j))\n",
    "\n",
    "print('Done! Extracted all the information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3abe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(1,214):\n",
    "    \n",
    "    with open('houses_location.csv','a',encoding ='utf8',newline = '') as file:\n",
    "        \n",
    "        writer_file = writer(file)\n",
    "        header = ['ID','Location']\n",
    "        writer_file.writerow(header)  \n",
    "        \n",
    "        print('scraping page {}....'.format(j))\n",
    "\n",
    "        res = sess.get('https://cbjamaica.com/property-search?page={}&limit=12&rent_sale=both&bed=&bath=&property_type=House&parish=&town=&currency=&keyword=&range=&agent=&agent_name=&similar=&tagOnly=&search_field=&idSearch=&property_amenities=&statuses=&price_range=&lattitude=&longitude='.format(j))\n",
    "\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "        car_el = soup.select('div#products>div>div>div:nth-child(3)>div>h2>a')\n",
    " \n",
    "        links = []\n",
    "        for i in car_el:\n",
    "            link = i.get('href')\n",
    "            links.append(link)\n",
    "\n",
    "\n",
    "        for k in links:\n",
    "            print('scraping link {}'.format(k))\n",
    "            \n",
    "            res = sess.get(k)\n",
    "            \n",
    "            soup = BeautifulSoup(res.text,'html.parser')\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                loc = soup.select('div.sale-offer-part>p')[0].text.strip()\n",
    "                loc = \" \".join(loc.split())\n",
    "            except:\n",
    "                loc = \"NA\"\n",
    "                \n",
    "            try:\n",
    "                ad_id = soup.select('div.sale-offer-part>h1>span')[0].text.strip()\n",
    "            except:\n",
    "                ad_id = \"NA\"\n",
    "                \n",
    "            rows = [ad_id,loc]\n",
    "            \n",
    "            writer_file.writerow(rows)\n",
    "            \n",
    "print('Done! All locations extracted')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806989cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sess.get('https://cbjamaica.com/property-details/MLS-59443')\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "try:\n",
    "    loc = soup.select('div.sale-offer-part>p')[0].text.strip()\n",
    "    loc = \" \".join(loc.split())\n",
    "except:\n",
    "    loc = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7006e",
   "metadata": {},
   "source": [
    "# scraping House Images link for https://cbjamaica.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c320c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,214):\n",
    "\n",
    "    print('scraping page {}....'.format(j))\n",
    "\n",
    "    res = sess.get('https://cbjamaica.com/property-search?page={}&limit=12&rent_sale=both&bed=&bath=&property_type=House&parish=&town=&currency=&keyword=&range=&agent=&agent_name=&similar=&tagOnly=&search_field=&idSearch=&property_amenities=&statuses=&price_range=&lattitude=&longitude='.format(j))\n",
    "\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "    car_el = soup.select('div#products>div>div>div:nth-child(3)>div>h2>a')\n",
    "\n",
    "    links = []\n",
    "    for i in car_el:\n",
    "        link = i.get('href')\n",
    "        links.append(link)\n",
    "  \n",
    "        \n",
    "    for i in links:\n",
    "        print('getting images from {}'.format(i))\n",
    "        res = sess.get(i)\n",
    "\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "        try:\n",
    "            ad_id = soup.select('div.sale-offer-part > h1>span')[0].text.strip()\n",
    "        except:\n",
    "            ad_id = \"NA\"\n",
    "        \n",
    "        image = soup.select('div.slick-track>img')\n",
    "\n",
    "\n",
    "        try:  \n",
    "            for i in image:\n",
    "\n",
    "                im_el = i.get('src')\n",
    "                with open('img_cbjam.csv','a',encoding ='utf8',newline = \"\") as file:\n",
    "                    writer_file = writer(file)\n",
    "                    img_id= [ad_id,im_el]\n",
    "                    writer_file.writerow(img_id)\n",
    "                \n",
    "        except:\n",
    "            im_el = \"NA\"\n",
    "            with open('img_cbjam.csv','a',encoding ='utf8',newline = \"\") as file:\n",
    "                writer_file = writer(file)\n",
    "                img_id= [ad_id,im_el]\n",
    "                writer_file.writerow(img_id)\n",
    "\n",
    "                \n",
    "                \n",
    "print('Done! Got all the image links')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79227170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = xlsxwriter.Workbook('new.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "name = ['ram','shyam','kohli']\n",
    "runs = [34,45,67]\n",
    "\n",
    "with open('new.xlsx') as file:\n",
    "    \n",
    "    worksheet.write(0,0,'names')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'dataset':['A', 'B', 'C', 'D', 'E']})\n",
    "df2 = pd.DataFrame({'dataset':[3, 12, 18, 21]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5680dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('dataframes.xlsx') as writer:\n",
    "    \n",
    "    df1.to_excel(writer, sheet_name = 'first', index = False)\n",
    "    df2.to_excel(writer, sheet_name = 'second', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8015987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('dataframes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ca859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comb'] = df['Prop Ad']+\", \" + df['City'] +\", \" +df['State']+ \", \"  + df['County']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ef129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3799a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a34cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f74a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ec0f3e",
   "metadata": {},
   "source": [
    "## Getting data for car ads posted after 29.08.2022 - https://www.jacars.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6565218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f22e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(10,11):\n",
    "    \n",
    "    res = sess.get('https://www.jacars.net/cars/?page={}&ordering=newest'.format(j))\n",
    "    \n",
    "    print('scraping page {}....'.format(j))\n",
    "\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "\n",
    "    other_el = soup.select('ul.list-simple__output.js-list-simple__output>li.announcement-container')\n",
    "\n",
    "\n",
    "\n",
    "    master_link = []\n",
    "    for i in other_el:\n",
    "        \n",
    "        date = i.select('div.list-announcement-block > div.announcement-block-text.announcement-block__text > div > div.announcement-block__date')[0]\n",
    "        regexobj = re.compile(r'Today|Yesterday|31.08.2022|30.08.2022',re.I)\n",
    "\n",
    "        car_date = date.text.strip()\n",
    "        mat = regexobj.search(car_date)\n",
    "        if mat != None :\n",
    "            half_link = i.select('div.list-announcement-block > div.announcement-block-text.announcement-block__text > div > a')[0]\n",
    "            full_link = urljoin('https://www.jacars.net',half_link.get('href'))\n",
    "            master_link.append(full_link)\n",
    "\n",
    "    \n",
    "    \n",
    "    with open('jacars_latest.csv','a',encoding ='utf8',newline = '') as file:\n",
    "        writer_file = writer(file)\n",
    "        header = ['ID','Title','address','price','phone','description','year','alarm','seats','airbag','colour','gearbox','mileage','body type','fuel type','drivetrain','engine size','air conditioning','right hand drive','power windows']\n",
    "        writer_file.writerow(header)  \n",
    "        \n",
    "        \n",
    "        for link in master_link:\n",
    "            print('scraping car link {}....'.format(link))\n",
    "\n",
    "            res = sess.get(link)\n",
    "\n",
    "            soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "            ad_id = soup.find('span',attrs = {'class':'number-announcement'}).text.strip()\n",
    "\n",
    "            car_title = soup.find('h1',attrs = {'id':'ad-title'}).text.strip()\n",
    "            \n",
    "            try:\n",
    "                address = soup.find('span',attrs = {'itemprop':'address'}).text.strip()\n",
    "            except NoSuchElementException:\n",
    "                address = \"NA\"\n",
    "            \n",
    "            try:\n",
    "                price = soup.find('div',attrs = {'class':'announcement-price__cost'}).text.strip().replace('\\n                                \\n                                  \\n\\n                                        \\n                                          ',\"\")\n",
    "            except NoSuchElementException:\n",
    "                price = \"NA\"\n",
    "                \n",
    "            try:\n",
    "                ph_no = soup.find('span',attrs = {'class':'phone-author-subtext__main'}).text.strip()\n",
    "            except NoSuchElementException:\n",
    "                ph_no = \"NA\"\n",
    "                \n",
    "            try:\n",
    "                descr = soup.select('div.announcement-description > p')[0].text.strip()\n",
    "            except:\n",
    "                descr = \"NA\"\n",
    "                \n",
    "            lis = soup.select('ul.chars-column>li')\n",
    "            \n",
    "            year =\"\"\n",
    "            alarm =\"\"\n",
    "            seats=\"\"\n",
    "            airbag=\"\"\n",
    "            colour =\"\"\n",
    "            gearbox =\"\"\n",
    "            mileage =\"\"\n",
    "            body_type=\"\"\n",
    "            fuel_type=\"\"\n",
    "            drivetrain =\"\" \n",
    "            engine_size =\"\"\n",
    "            air_conditioning=\"\"\n",
    "            right_hand_drive= \"\"\n",
    "            power_windows=\"\"\n",
    "            \n",
    "            \n",
    "            for i in lis:\n",
    "                obj = re.compile(r'Year:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match != None:\n",
    "                    year = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Alarm:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    alarm = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Seats:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    seats= i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Airbag:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    airbag= i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Colour:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    colour = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Gearbox:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    gearbox = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Mileage:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    mileage = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Body type:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    body_type= i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Fuel type:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    fuel_type= i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Drivetrain:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    drivetrain = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Engine size:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    engine_size = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Air conditioning:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    air_conditioning = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Right hand drive:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    right_hand_drive = i.text.strip()\n",
    "\n",
    "                obj = re.compile(r'Power windows, doors:',re.I)\n",
    "                match = obj.search(i.text)\n",
    "                if match !=None:\n",
    "                    power_windows= i.text.strip()\n",
    "        \n",
    "            if year == \"\":\n",
    "                year = 'NA'\n",
    "            if alarm ==\"\":\n",
    "                alarm = 'NA'\n",
    "            if seats == \"\":\n",
    "                seats = 'NA'\n",
    "            if airbag == \"\":\n",
    "                airbag = 'NA'\n",
    "            if colour == \"\":\n",
    "                colour = 'NA'\n",
    "            if gearbox == \"\":\n",
    "                gearbox = 'NA'\n",
    "            if mileage== \"\":\n",
    "                mileage = 'NA'\n",
    "            if body_type == \"\":\n",
    "                body_type = 'NA'\n",
    "            if fuel_type == \"\":\n",
    "                fuel_type= 'NA'\n",
    "            if drivetrain == \"\":\n",
    "                drivetrain = 'NA'\n",
    "            if engine_size == \"\":\n",
    "                engine_size = 'NA'\n",
    "            if air_conditioning == \"\":\n",
    "                air_conditioning = 'NA'\n",
    "            if right_hand_drive== \"\":\n",
    "                right_hand_drive = 'NA'\n",
    "            if power_windows == \"\":\n",
    "                power_windows= 'NA'\n",
    "\n",
    "            \n",
    "            rows = [ad_id,car_title,address,price,ph_no,descr,year,alarm,seats,airbag,colour,gearbox,mileage,body_type,fuel_type,drivetrain,engine_size,air_conditioning,right_hand_drive,power_windows]\n",
    "\n",
    "            writer_file.writerow(rows)\n",
    "            \n",
    "            print('writing link {} finished, moving to next link'.format(link))\n",
    "        \n",
    "    print('Finished writing page {}, moving on to the next page'.format(j))\n",
    "\n",
    "print('Done! Extracted all the information')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = other_el[0].select('div.list-announcement-block > div.announcement-block-text.announcement-block__text > div > div.announcement-block__date')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d15bab",
   "metadata": {},
   "source": [
    "## Getting image links for car ads posted after 29.08.2022 from https://www.jacars.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e69a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(1,11):\n",
    "    \n",
    "    res = sess.get('https://www.jacars.net/cars/?page={}&ordering=newest'.format(j))\n",
    "    \n",
    "    print('scraping page {} ....'.format(j))\n",
    "\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "\n",
    "    other_el = soup.select('ul.list-simple__output.js-list-simple__output>li.announcement-container')\n",
    "\n",
    "\n",
    "\n",
    "    master_link = []\n",
    "    for i in other_el:\n",
    "        \n",
    "        date = i.select('div.list-announcement-block > div.announcement-block-text.announcement-block__text > div > div.announcement-block__date')[0]\n",
    "        regexobj = re.compile(r'Today|Yesterday|31.08.2022|30.08.2022',re.I)\n",
    "\n",
    "        car_date = date.text.strip()\n",
    "        mat = regexobj.search(car_date)\n",
    "        if mat != None :\n",
    "            half_link = i.select('div.list-announcement-block > div.announcement-block-text.announcement-block__text > div > a')[0]\n",
    "            full_link = urljoin('https://www.jacars.net',half_link.get('href'))\n",
    "            master_link.append(full_link)\n",
    "\n",
    "            \n",
    "    for i in master_link:\n",
    "        print('getting images from {}'.format(i))\n",
    "        res = sess.get(i)\n",
    "\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "        ad_id = soup.find('span',attrs = {'class':'number-announcement'}).text.strip()\n",
    "        \n",
    "        image = soup.select('div.announcement__images>img[data-full]')\n",
    "\n",
    "        try:  \n",
    "            for i in image:\n",
    "\n",
    "                im_el = i.get('src')\n",
    "                with open('jacars_latest_images.csv','a',encoding ='utf8',newline = \"\") as file:\n",
    "                    writer_file = writer(file)\n",
    "                    img_id= [ad_id,im_el]\n",
    "                    writer_file.writerow(img_id)\n",
    "                \n",
    "        except:\n",
    "            im_el = \"NA\"\n",
    "            with open('jacars_latest_images.csv','a',encoding ='utf8',newline = \"\") as file:\n",
    "                writer_file = writer(file)\n",
    "                img_id= [ad_id,im_el]\n",
    "                writer_file.writerow(img_id)\n",
    "\n",
    "print('Done! Got all the image links')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc7c54",
   "metadata": {},
   "source": [
    "### Scraping https://iaccmanagement.govoffice2.com/index.asp?SEC=65D98D93-45C5-4634-83D5-1F7E6539A587&pri=0 (Iframe Switch involved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fe219",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Edge()\n",
    "driver.get('https://iaccmanagement.govoffice2.com/index.asp?SEC=65D98D93-45C5-4634-83D5-1F7E6539A587&pri=0')\n",
    "\n",
    "iframe = driver.find_element_by_tag_name('iframe') #------------------->locate the iframe\n",
    "\n",
    "driver.switch_to.frame(iframe) #--------------------> switch to iframe\n",
    "\n",
    "emp_el = driver.find_elements_by_css_selector('tbody>tr>td')\n",
    "\n",
    "for i in emp_el:\n",
    "    name = i.find_element_by_css_selector('span>strong').text.strip()\n",
    "    \n",
    "    role = i.find_element_by_css_selector('span').text.strip()\n",
    "    try:\n",
    "        email = i.find_element_by_css_selector('a>span').text.strip()\n",
    "    except:\n",
    "        email = \"NA\"\n",
    "    rows = [name,role,email]\n",
    "    \n",
    "    with open('employee.csv','a',encoding = 'utf8',newline = \"\") as file:\n",
    "        writer_file  = writer(file)\n",
    "        writer_file.writerow(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fbab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.firstpost.com/opinion-news-expert-views-news-analysis-firstpost-viewpoint/congress-claim-on-ins-vikrant-is-spurious-nehru-rajiv-gandhi-used-navy-warships-aircraft-carriers-for-vacation-11168821.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb571f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_article = Article(url,language = \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_article.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_article.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc6c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92490527",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Edge()\n",
    "url = 'https://www.google.com/maps/search/real+estate/@18.0482622,-77.7305469,9z/data=!3m1!4b1'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#lis_el = driver.find_element_by_css_selector('div[aria-label=\"Results for real estate\"]')\n",
    "#driver.execute_script('arguments[0].scrollIntoView();',lis_el)\n",
    "\n",
    "\n",
    "\n",
    "#lis_el = driver.find_element_by_css_selector('div.m6QErb.DxyBCb.kA9KIf.dS8AEf.ecceSd')\n",
    "\n",
    "#ActionChains(driver).move_to_element(lis_el).scroll\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SCROLL_PAUSE_TIME = 3\n",
    "last_height = driver.execute_script('return document.querySelector(\\'[aria-label=\"Results for real estate\"]\\').scrollHeight')\n",
    "print(last_height)\n",
    "\n",
    "#driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight',lis_el)\n",
    "while True:\n",
    "    \n",
    "\n",
    "   \n",
    "    #Scroll down to bottom\n",
    "    driver.execute_script('document.querySelector(\\'[aria-label=\"Results for real estate\"]\\').scrollTo(0,1000000);')\n",
    "    \n",
    "    #Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    \n",
    "   \n",
    "    #print('ok')\n",
    "\n",
    "    #Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script('return document.querySelector(\\'[aria-label=\"Results for real estate\"]\\').scrollHeight')\n",
    "    print(new_height)\n",
    "    if new_height == last_height:\n",
    "        break      \n",
    "    last_height = new_height\n",
    "\n",
    "\n",
    "lis_el = driver.find_elements_by_css_selector('div.m6QErb.DxyBCb.kA9KIf.dS8AEf.ecceSd > div>div[role=\"article\"]')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_top_but = driver.find_element_by_css_selector('button[aria-label = \"Back to top\"]')\n",
    "ActionChains(driver).click(back_top_but).perform()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lis_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf969e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in lis_el:\n",
    "    \n",
    "    driver.execute_script('arguments[0].scrollIntoView();',i)\n",
    "    \n",
    "    \n",
    "\n",
    "    ActionChains(driver).click(i).perform()\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    name = driver.find_element_by_css_selector('h1.DUwDvf.fontHeadlineLarge>span').text\n",
    "    print(name)\n",
    "\n",
    "    try:\n",
    "        addr = driver.find_element_by_css_selector('button[aria-label ^=\"Address\"]').get_attribute('aria-label')\n",
    "    except:\n",
    "        addr = \"NA\"\n",
    "        \n",
    "    try:   \n",
    "        website = driver.find_element_by_css_selector('a[aria-label ^=\"Website\"]').get_attribute('href')\n",
    "    except:\n",
    "        website = \"NA\"\n",
    "    \n",
    "    try:\n",
    "        phone = driver.find_element_by_css_selector('button[aria-label^=\"Phone\"]').get_attribute('aria-label')\n",
    "    except:\n",
    "        phone = \"NA\"\n",
    "        \n",
    "    try:\n",
    "        plus_code = driver.find_element_by_css_selector('button[aria-label^=\"Plus code\"]').get_attribute('aria-label')\n",
    "    except:\n",
    "        plus_code = \"NA\"\n",
    "    \n",
    "    try:\n",
    "        reviews = driver.find_element_by_css_selector('button[aria-label*=\"reviews\"]').get_attribute('aria-label')\n",
    "    except:\n",
    "        reviews = \"NA\"\n",
    "    \n",
    "    try:\n",
    "        stars = driver.find_element_by_css_selector('div[role = \"button\"]>span>span>span[aria-label*=\"stars\"]').get_attribute('aria-label')\n",
    "    except:\n",
    "        stars = \"NA\"\n",
    "    \n",
    "    \n",
    "    lis = [name,addr,website,phone,plus_code,reviews,stars]\n",
    "    with open('real_estate.csv','a',encoding  = 'utf8',newline = \"\") as file:\n",
    "        writer_file = writer(file)\n",
    "        writer_file.writerow(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Edge()\n",
    "url = 'https://www.google.com/maps/search/real+estate/@18.0482622,-77.7305469,9z/data=!3m1!4b1'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "element = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e473e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d073ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037056fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
